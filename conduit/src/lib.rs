// We have a bunch of nodes
// - They can set up threads/events to wake themselves up
// - They can be notified when data has been modified
// - They can produce new pieces of data
//
// We have lots of data objects
// - They have metadata that lists the input data used to generate them
// - All have GUID IDs
// - They can be stored/retrieved.. maybe content addressable? we probably need to ensure they are
//   unique if we want to be able to delete them when they are no longer used. Actually, if we want
//   to cache/reuse results, they should not be unique. Maybe we have immutable, non-changing data
//   in the S3-like store that has unclear/shared ownership, and we have metadata for it all
//   somewhere else where we can have tiny records about repeated jobs (even if the inputs were the
//   same) in something like redis.
// - Or we can do some other cleanup method like randomly swimming through them and verifying that
//   they are still in use
// - Invalidation idea: Randomly do expensive checks to see if we can evict data, and stop when some
//   percentage of checks decide the data isn't stale
// - Other invalidation idea: Scan metadata to see what is referenced, update reference counts.
//   Then scan S3/ceph and remove anything with a 0 reference count. Must be able to regenerate anything
//   that got removed in case we race and need it again. Full steps to regen must be in metadata.
// - Should plan to store binary payloads separately from metadata:
//   - If needs to be distributed: S3-like (ceph), reddis for metadata/event queues
//   - If doesn't need distributed: LMDB/minio/local file system, local locking/channels
//   - Final export to LMDB
// - Maybe we need to have the concept of grouping pieces of data? usecase: shaders. Not sure if this
//   setup as a one-off where needed or a core feature. Grouping would be useful to ensure we capture
//   changes atomically
//.3
//
// - dir_watcher will fire change/remove events for files
// - if a file matches any data query, upload it
// - update metadata

// Process for ingest and processing:
// - We have a list of directories that we will watch
// - We maintain a list of the full file system state (manifest)
// - When we decide to start building (perhaps when it stops changing for some amount of time) we
//   try to import everything
//   - Support custom imports that for example, parse json and produce bincode
//   - Also support scraping the whole file
// - All files are pushed to storage
// - Publish events when files are ready to be processed
//   - Some things we can force to process locally
//   - Other things we would process remotely (their results are saved to S3)
// - When a processing step completes, we return new binary blobs (just metadata, the payload is left in S3)
// - The full list of binary blobs that came from the files we had at the start form a blob manifest
// - As we add new things to that list, additional processes may become ready
// - Some blobs can be exported, those are placed into a binary file or LMDB
// - Use existing distill code to push assets changes to game client
// Optimization
// - We can simulate out all of the above into a linear list of steps
// - We re-evaluate the steps as new data becomes available
// Tagging
// - We can use key/value to store metadata along with the artifacts (binary blobs)
// - A likely key to filter by would be the process the produced the data
// We may need a way to know that a file references other files by path so we know there is a dependency
// - what would it reference? the import asset? an artifact?
// - might need some way to search all artifacts by tag/file path (the file path being all data files that
//   contributed to the output
// Culling - If we build the entire sequence of what will be built and know what will be exported, we can
//   skip pointless things that aren't exported
//   - This requires knowing what an asset will produce without importing/processing it
// Caching processing results - Every time we write processed data to S3, we hash the inputs that
//   produced it. When we start processing, we check if the hash already exists.

mod dir_watcher;
mod error;

use base::hashing::{HashMap, HashSet};
use std::path::{Path, PathBuf};
use std::sync::Arc;

const PROCESS_NAME_INGEST: &str = "ingest";
const PROCESS_NAME_TRANSFORM: &str = "transform";
const PROCESS_NAME_EXPORT: &str = "export";

//
struct PipelineData {}

// tags? data type? location? guid? recipe to recreate? how it was created? Reference to full data?
struct PipelineDataMeta {}

// a query for data sources
// examples:
//  - all data produced by node X (rely on data found tagged as generated by node X)
//  - affinity to run on particular hardware nodes?
//  -
//
// Maybe some nodes monitor files to wake themselves?

trait PipelineProcess {
    fn setup();
}

//struct PipelineProcess {
//    id: PipelineProcessId
//}
//
//impl PipelineProcess {
//
//}

//struct PipelineManager {
//    pipelines: PipelineProcess
//}
//
//impl PipelineManager {
//
//}

struct MachineId(String);
struct ProcessInstanceId(uuid::Uuid);
struct PipelineProcessId(uuid::Uuid);

struct DataQuery {
    from_pipeline_process: PipelineProcessId,
}

struct DataStore {
    data: HashMap<uuid::Uuid, Arc<Vec<u8>>>,
}

impl DataStore {
    fn insert(data: Vec<u8>) {}
}

// This is shared across all locally processes
//struct FileWatchService {
////    paths: HashSet<PathBuf>,
////    n: notify::RecommendedWatcher
////}

//impl FileWatchService {
//    fn add_watch(path: &Path) {
//
//    }
//}

//struct IngestFilesProcess {
//    path: PathBuf,
//    recursive: bool,
//
//}

//impl ReadFilesProcess {
//
//}

pub fn run() {
    //TODO: Do we want to use futures_channel, crossbeam, or something else?
    let (tx, mut rx) = futures_channel::mpsc::unbounded();
    let to_watch = vec![PathBuf::from("/Users/philipd/dev/rust/rafx/demo/assets")];
    let mut watcher =
        dir_watcher::DirWatcher::from_path_iter(to_watch.iter().map(|p| Path::new(p)), tx).unwrap();
    std::thread::spawn(move || watcher.run());
    loop {
        let next = rx.try_next();
        if let Err(e) = next {
            continue;
        }
        dbg!(next);
    }

    let d = Vec::<u8>::default();

    //let mut pipeline = PipelineManager::default();
    //    // Ingest process
    //    pipeline.add_process(PipelineProcess::new(input_query, what_do_do));
    //    // Transform process
    //    pipeline.add_process(PipelineProcess::new(input_query, what_do_do));
    //    // Export process
    //    pipeline.add_process(PipelineProcess::new(input_query, what_do_do));
}
